---
type: "post"
title: "机器学习课程学习笔记"
author: "horizon"
category: "Tech"
date: "2025-03-19"
slug: "/Tech_16"
postImage: "./img/Tech_16.jpg"
metaDescription: "记录了机器学习课程的学习笔记。包含了机器学习的基础知识、监督学习、无监督学习、强化学习、线性回归、决策树、随机森林、贝叶斯分类等内容。"
---

## 2.18 机器学习基础

1.  人工智能：能够处理⼀般需要⼈类知识和智⼒介⼊任务的计算机系统

    - 机器学习：从若⼲输⼊输出样例中学习数据规律的统计技术（机器学习是一类通过从数据中发掘潜在规律来不断优化自身性能，以实现特定目标的计算机算法。）
    - 深度学习：⼀种⾃动学习数据表征的机器学习技术

2.  传统编程：包含太多决策规则（比如关键词）

3.  机器学习的核心要素

    - 数据 (经验)
    - 模型 (假设)
    - 损失函数(目标)
    - 优化算法(提升)

4.  归根结底，机器学习是什么？

    - 定义：机器学习是一种人工智能技术，使机器能够在无需显式人工编程的情况下，通过从数据中自动发现潜在的统计规律（模型），并不断优化自身性能（优化），以实现特定的目标（如最小化损失函数）。

5.  推广到其他类型的数据

    - 向量化/特征提取(转化成高维向量空间中的点)

6.  以下人类学习行为分别对应哪种机器学习要素？

    1. 查文献 : 数据
    2. 考试 : 损失函数
    3. 错题总结 : 优化算法
    4. 挂科 : 没有

7.  如何构建一个机器学习系统？

    1. 考虑数据可⾏性和规模
       - 能获得多少数据? 需要多少代价 (时间、算⼒、⼈⼒成本)?
    2. 选择输⼊数据的合理表示形式
       - 数据预处理,特征提取,等等.
    3. 选择合理的模型假设
    4. 选择合适的损失函数
    5. 选择或设计⼀个学习算法

8.  机器学习分类

    - 监督学习：学习器的每个输入样本都有一个目标输出标签（有 “⽼师”的学习举例：老师教学生识别各种类型的动物。练习题提供标准答案）
      - 分类问题，回归问题
    - ⽆监督学习：Training examples as input patterns, with no associated output.
      - 聚类问题，降维问题，数据的概率密度估计，数据⽣成，异常检测
    - 强化学习：通过与环境交互来学习。学习状态(states)到动作(actions)的映射，以最大化长期奖励(reward)。（例: 练习题只给总分不给参考答案(刷题模式)）

9.  监督学习 vs.无监督学习

    - 监督学习
    - 数据:(x, y) x 表示数据,y 表示标签
    - 学习⽬标:学习函数映射 x →y
    - 典型任务:分类、回归、⽬标检测、语义分割等

    - 无监督学习
    - 数据:x 表示数据,⽆标签
    - 学习⽬标:学习数据隐含或潜在的结构
    - 典型任务:聚类、降维、概率密度估计等

10. 泛化性

    - 学习后的分类器能否处理没有见过的水果？
    - 学习的泛化性问题:模型应该学习普遍的规律还是记住特定细节?
    - 左边的模型更好，不复杂，泛化性更好

11. 欠拟合 vs 过拟合

    - 欠拟合–因模型不够复杂无法刻画真实规律，模型假设可能不成立
    - 过拟合–因模型过于复杂导致记住数据的细节(如随机噪声)而不是潜在的规律

## 2.25 机器学习基础

1. 避免过拟合（常见的避免过拟合方法）

   - 增加训练数据
   - 正则化(惩罚模型复杂度)
   - 数据划分&交叉验证(训练时用未知数据测试确保泛化能力)
   - 早停
   - 引入先验知识(如贝叶斯先验)

2. 措施一：数据划分

   - 模型拟合的是总体数据的分布而不仅仅是训练集的分布.
   - 为了评估泛化误差，需要在训练的时候保留一部分未知数据。
     - 数据划分(Hold-Out):
       - 训练集(e.g.,50%)
       - 验证集(e.g.,25%)
       - 测试集(e.g.,25%)

3. 措施二：交叉验证(CrossValidation)

   - 如何既划分数据，又能充分利用数据训练? (特别是当数据规模较⼩的时候)
   - K-折交叉验证
     - 将数据集分成 K 份
     - 依次将每份数据作为验证集，其余数据作为训练集
     - 计算 K 次验证结果的平均值

4. 措施三:早停(EarlyStop)

   - 在有过拟合迹象时及时停止训练

5. 措施四:正则化(Regularization)

   - 对参数增加惩罚项防止模型过拟合训练数据。
   - L1 正则化: $L1 = \lambda \sum_{i=1}^{n} |w_i|$
   - L2 正则化: $L2 = \lambda \sum_{i=1}^{n} w_i^2$

6. No free lunch 理论

   - 没有通用的最好的模型
   - 不同模型需要适配数据特征和应用

7. 机器学习中的回归问题

   - 回归问题:预测连续值

8. 线性回归

   - 使用一个线性函数来拟合数据
   - 损失函数:均方误差(Mean Squared Error)
     - $MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2$
   - 优化算法:梯度下降(Gradient Descent)
     - $w = w - \alpha \frac{\partial L}{\partial w}$

9. 线性回归的矩阵形式

   - 转化为矩阵求解问题

10. 正则化

    - 问题:XTX 可能不可逆
    - 解决方法:正则化(对损失函数引入一个惩罚项)
    - 对模型的惩罚变相地相当于数据的增强

## 2.28 决策树&随机森林&贝叶斯分类

1. 介绍了一下线性回归的具体算法实现

2. 然后讲了一下决策树模型

3. 回归 vs.分类

   - 回归:(predicts real-valued labels)
   - 分类:(predicts categorical labels)

4. 分类问题是机器学习处理的主要任务

5. 决策树和随机森林是最简单的分类算法

6. 分类=寻找一个决策边界，将特征空间划分成 2 个部分（每部分代表一个水果类型）

   - 非线性(cannotbelinearly separated)
   - 可能非数值 (e.g.,“male”, “female”)
   - 但是，沿着某些维度局部线性可分

7. 决策树的基本思想

   - 分而治之
   - 划分数据属性
   - 创建 if-then 规则

8. 决策树的模型结构,决策树包含三种类型的节点:

   1. 根节点(root node)
   2. 中间节点(Internal nodes)
   3. 叶子节点(Leaf nodes，terminal nodes)

   - 每个叶子节点对应一个类别标签
   - 每个非叶子节点包含属性测试条件， 用来根据属性不同特征划分数据.

9. 如何构建(训练)一棵决策树: 自顶向下分治学习

   1. 构造一个根节点，包含整个数据集.
   2. 选择一个最合适的属性
   3. 根据选择属性的不同取值，将当前节点的样本划分成若干子集；
   4. 对每个划分后的子集创建一个孩子节点，并将子集的数据传给该孩子节点；
   5. 递归重复 2~4 直到满足停止条件.

10. 如何高效率的构建决策树?

    - 选择每个属性 X 的时候，尽可能最大化标签 Y 的纯度
    - 决策树将数据逐步分割成越来越小的子集。最理想的情况是叶节点对应
      的样本属于同一类别(节点的标签纯净)。

11. 如何选择属性以最大化标签的纯度?

    - 选择 X 以最大化信息增益、Gini 系数等
    - 每⼀种指标对应⼀种决策树构造算法

12. 按照不同的属性划分原则，有以下常⻅算法：

    - ID3 (基于信息增益)
    - CART(基于 Gini Index)
    - C4.5 (基于信息增益率)

13. ID3 算法

    - 试着对每个属性进行划分，看看哪个划分效果“最好”
    - 评估样本的整体信息熵的降低程度
    - H(D) - H(D|X) = 信息增益 Gain(D, X)（即划分前后信息熵的差值）

14. 举例

    - Step1– 创建根节点
    - Step2– 计算每个(⼦)数据集的熵.
    - Step3– 对每个属性计算信息增益 IG,选择具有最⼤ IG 的属性
    - Step4–将最⼤ IG 的属性赋给当前(根)节点。对每个属性取值扩展⼀个分⽀⽤于创建新节点。
    - Step5–将数据集按选中的属性取值进⾏拆分，然后从数据集中移除该属性。
    - Step6–对每个划分后的⼦数据集，重复 2-5 步直到满⾜停⽌条件（递归）

15. 停止划分的条件

    - 纯度 – 叶⼦节点包含的样本均属于同⼀类
    - 数据量过小 – 叶⼦节点包含的样本数⼩于⼀个阈值
    - 没有属性可分(ID3)

16. 算法优点

    - 易理解, 可解释性, 便于可视化分析,
    - 数据预处理要求低,
    - 可以轻易处理非线性边界,
    - 数据驱动，可以以任意精度拟合训练集

17. 算法缺点

    - 容易过拟合
    - 某些噪声样例总能被树涵盖并学习

18. 随机森林

    - 将数据随机划分子集，各自构建决策树，将结果合并。
    - 往往比决策树更准确（因为随机划分的时候、会把噪声给去掉）
    - 训练速度快, 容易并行化 (训练时树与树之间是相互独立的).
    - 可以处理高维(feature 很多)数据，并且不用做特征选择 (因为特征子集是随机选择的).
    - 可以处理缺失属性.
    - 结果易解释 (i.e.,在训练完后，能够给出哪些 feature 比较重要).
    - 在训练过程中，能够检测到 feature 间的互相影响。
    - 对于不平衡的数据集来说，它可以平衡误差。

19. 贝叶斯分类

    - 就是这两类数据不能用直接进行划分，但是符合一定的概率分布，然后通过概率分布来进行分类，刻画这个模型（比如正态分布）
    - 就是一开始、我们把这个数据认为是从线来产生的、然后现在把这个数据看作是投色子的概率来产生的
    - 参数就是 XY 的联合概率分布，然后通过这个联合概率分布来进行分类
    - 损失函数就是最大后验概率

## 3.4 朴素贝叶斯&KNN

1. 朴素贝叶斯分类（根据上面的概率思想、进行分类）

   - 一开始的场景是，我们分类邮件是否是垃圾邮件
   - P(C1) = 85%(是正常邮件的概率)(先验概率)
   - P(C2) = 15%(是垃圾邮件的概率)(先验概率)
   - X：邮件中的词（特征）（比如钱、免费、优惠）
   - P(X|Ci) :似然概率，即在某种邮件下，某个词出现的概率
   - P(Ci|X) :后验概率，即在某个词下，这个邮件是垃圾邮件的概率
   - Decision rule: decide C1 if p(x|C1)P(C1) > p(x|C2)P(C2)

2. 如果是多维的、就是多个特征、就是多个词，多个随机变量（但是也可以把它先看作是一个变量，然后通过联合概率进行计算，就是一个概率分布表，统计出来就可以做分类了）

   - 问题在于：这个联合概率分布表、太大了、过于复杂难以计算

3. 朴素贝叶斯假设

   - 在我某一个类已知的情况下、这些特征是相互独立的
   - 就不用把所有的概率都理一遍了
   - 即 P(X|Ci) = P(X1|Ci)P(X2|Ci)P(X3|Ci)P(X4|Ci)P(X5|Ci)

4. How to estimate each P(c)?

   - 直接统计每个类别的概率

5. How to estimate P(xi|c) for each c?

   - P(xi|c) = count(xi, c) / count(c)
   - count(xi, c) : 在类别 c 下，特征 xi 出现的次数
   - count(c) : 类别 c 出现的次数
   - 本质上就是数数

6. 但是有一个问题、有可能在训练集中、有些词在某个类别下没有出现、这个时候就会出现概率为 0 的情况（zero-count）

   - 解决方法：拉普拉斯平滑（即给每个 value 加一个 1）
   - P(xi|c) = (count(xi, c) + 1) / (count(c) + 1)

7. 应用场景

   - 文本分类（因为容易计数，每一个单次就是一个属性）
   - 实时预测（因为计算量小，快）
   - 多分类问题（因为类别的数量是可以指定为多个的）

8. 垃圾邮件里的例子

   - 先算似然概率和先验概率
   - 然后通过贝叶斯公式计算后验概率，然后比对后进行分类

9. 优点：

   - 训练和预测速度快
   - 多分类问题效果很好
   - 对缺失数据不太敏感，易于维护、可以很快地处理新数据（只需要增加或者删改计数）

10. 缺点：

    - 假设过于简单，会导致欠拟合（但是实际上的效果还是可以的）
    - 就是有时候词之间的出现也是有关系的、但是通过假设、就是没有考虑这个关系

11. KNN：不需要模型的算法

    - 思想：物以类聚，人以群分
    - 算法：就是根据相邻的几个邻居做一个投票，然后根据投票结果进行分类
    - 问题：如何去度量这个距离

12. 欧氏距离的缺点：当维度变高的时候、受属性之间的区别关系就不大了

13. 实际数据分布的特点和缺点：

    - 就是在每个维度、数据的分布是不一样的，会导致在某些维度上、学习的效果不好
    - 解决方法：对每个维度进行归一化处理

14. 归一化方法：

    - 最大最小归一化：$x_{new} = \frac{x - min}{max - min}$
    - Z-score 标准化：$x_{new} = \frac{x - \mu}{\sigma}$

15. Similarity vs. Distance（相似度和距离）

    - 相似度：越大越相似
    - 距离：越小越相似

16. Cosine Similarity

    - 余弦相似度：$cos(\theta) = \frac{A \cdot B}{||A|| \cdot ||B||}$
    - 可以用来度量两个向量的相似度

17. 算法流程：

    - 决定 K
    - 计算测试样本和训练样本的距离
    - 选择距离最近的 K 个样本
    - 对这 K 个样本进行投票
    - 根据投票结果进行分类

18. 如何决定这个最好的 K 值？

    - 维诺图：就是 K 值和误差率的关系图（所有点之间等距离的边界）
    - K=1 的时候、就是垂直平分线
    - K>1 的时候、就是一个多边形，也会有很多空白区域（就是离邻居的距离都是相等的区域）
    - K 比较小的时候、分离面会比较小、过拟合
    - K 比较大的时候、分离面会比较光滑、会比较自然（ppt 上有图）

19. 优点：

    - 简单、易于理解
    - 无需训练
    - 新的数据很容易被加入

20. 缺点：

    - 计算量大
    - 高度数据相关
    - 需要大量的内存
    - 需要正则化处理

## 3.11 逻辑斯蒂回归

1. 决策树：构造决策面

2. 朴素贝叶斯：基于数据的概率分布

3. KNN：基于距离的分类找决策面

4. 上面方法的共性：找决策面

5. 把所有决策交给数据：抛开决策面和几何意义（逻辑斯蒂回归，利用数学函数进行投票统计）

6. 判别函数：用来将数据分开（可以是概率密度、也可以不是、只要能分开就可以了）

7. 决策边界：就是判别函数的值为 0 的地方

8. 判别函数：首先想到线性函数

   - 好处：简单、易于理解、可解释、准确
   - 几何意义：就是一个分离的超平面（半平面）
   - 由此便诞生了感知机

9. 感知机：就是一个最简单的线性分类器

   - 按照数据在超平面的哪一边来进行分类
   - 训练：如果预测出错，则更新权重（移动超平面进行修正）
   - 缺点：因为这个函数是阶跃的 01 函数，所以难以进行优化
   - 因此引入了刻画这个数据属于哪一类的程度（概率）

10. 为了体现 y 和 1-y 之间的关系，对 y/(1-y)取了 log、并与 0 对比

    - 即为 logit 函数：$logit(p) = log(\frac{p}{1-p})$
    - 可验证即相当于线性的

11. sigmoid 函数：logit 函数的反函数

    - $sigmoid(x) = \frac{1}{1+e^{-x}}$
    - 作用：将线性函数的输出映射到 0-1 之间
    - 完美的概率拟合公式

12. 逻辑斯蒂回归

    - 使用了 sigmoid 函数的分类器
    - 根据预测出的概率值进行分类

13. 损失函数设计

    - 要最大化 log p 的值
    - 因为当 r=1 时候、cost=-log(y)、当 r=0 时候、cost=-log(1-y)
    - 因此使用交叉熵损失函数：$L(y, \hat{y}) = -ylog(\hat{y}) - (1-y)log(1-\hat{y})$
    - 也相当于要最大化似然函数

14. 优化方法：梯度下降

    - 求导：$\frac{\partial L}{\partial w} = (y-\hat{y})x$
    - 更新：$w = w + \alpha(y-\hat{y})x$
    - 不断优化损失函数

15. 多分类的时候，对函数进行了扩展

    - softmax 函数：$softmax(x) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$
    - 作用：将线性函数的输出映射到 0-1 之间
    - 每个输入称为 logits
    - 最终得到所有类下的概率分布
    - soft：函数可微
    - max：选择最大的数值（原来是不可微的，离散的，softmax 把它变成了可微的）

16. softmax 里面、标签是一个向量、预测值也是一个向量

## 3.14 SVM 支持向量机

1. 最简单的线性分类器：感知机（通过函数的统一化思想）

2. 逻辑斯蒂回归：（概率）进行分类

3. 前言：线性分类：可能会有很多根线、要找到更好的那根分割线（横着还是竖着）

   - 应该选择把两边的间距最大化的那根线（因为这样可以更好的泛化）

4. 数学推导：要把那个小 δ 消去，相当于把两个平面加起来，然后这两个平面加起来之后、就是原来分割的超平面（w 是未知量、是要学习的变量）

   - 可以把那个小 δ 令为+1 和 -1，然后就可以得到一个等式
   - margin = 2/||w||
   - 优化目标：最大化 margin
   - 最后要最大化这个 margin 等效于要最小化 1/2 的 ||w||（||w||是 w 的范数，即 w 的长度）
   - 第二个约束：要在两个超平面外的至少距离为 1

5. 支持向量机：

   - 即一个凸优化问题（二次优化问题）
   - minimize $\frac{1}{2}||w||^2$
   - subject to $y_i(w \cdot x_i + b) \geq 1$
   - 结构：还是一个感知机
   - 区别：损失函数变化了

6. 损失函数：

   - hinge loss：$L(y, \hat{y}) = max(0, 1-y\hat{y})$
   - 作用：当预测正确的时候、损失为 0(因为已经满足了)，当预测错误的时候、损失为 1-y\hat{y}（相当于关注线和边界上的点）
   - 优点：对异常值不敏感

## 3.18 MLP 多层感知机

1. 根据人脑的神经元模型、提出了神经网络

2. 感知机：

   - 一个输入函数
   - 一个激活函数
   - 一个损失函数
   - 可以看作一个最简单的线性模型

3. 感知机最基本的作用：分类，逻辑运算（但是单个感知机没法解决异或问题）

4. 转折：但是隐藏层的加入、可以解决异或问题（即非线性问题，通过含隐藏层的多层感知机进行解决）

5. 还有一个问题：原来的 01 激活函数太简单的、不能进行优化，因此引入了 sigmoid 函数（连续的、可微的）

6. 三层：

   - 输入层：输入数据
   - 隐藏层：给数据进行初步的线性转换和非线性的激活（通过 sigmoid），然后告诉下一层
   - 输出层：把小的感知机的结果进行汇总

7. 多层感知机：万能的拟合工具（可以拟合很多非线性函数）

   - MLP 也叫 Fully Connected Neural Network

8. MLP 在干啥：

   - 狗的例子：第一层、先在图片里面找最基本的特征（比如有没有三角形、有没有轮廓），然后在后面层判断进一步细致的特征（比如是不是有两个眼睛）
   - 非线性变换的例子：相当于线性变换（旋转等）和非线性变换（拉伸和拟合）的组合，对一个空间进行一个揉捏

9. MLP 的相较于决策树的优势：连续可微的

10. MLP 中如何训练？学习的参数是巨大的

    - 但是本质上：还是要算梯度

11. 损失函数（需要最小化）：

    - MSE：$L(y, \hat{y}) = \frac{1}{2}(y-\hat{y})^2$

12. 通过导数的链式规则、推导得出了可以通过反向传播的方法、来进行参数的更新

    - 反向传播：就是通过链式法则、把损失函数的导数传递到每一层（得到结果之后、然后再反向传播）

13. 前向传播：

    - 从输入层到输出层的过程

14. 所以反向传播里有两个步骤，分别为正向和反向，然后合并起来就是梯度

    - 正向（forward pass）：计算 δa/δw
    - 反向（backward pass）：计算 δL/δa
    - 乘积：δL/δw（即梯度）

## 3.25 MLP 多层感知机 & Language Model

1. 问题：反向传播中使用递归的方式、会导致计算量巨大

   - 解决方法：使用动态规划的方式、把计算结果存储起来
   - a：就是激活函数的输入
   - w：就是权重
   - L：就是损失函数

2. 因此：权重的更新就是输入信号和误差信号的乘积

   - 输入信号：δa/δw
   - 误差信号：δL/δa

3. 算法过程总结

   - 正向传播：计算每一层的 δa/δw
   - 反向传播：计算每一层的 δL/δa
   - 更新权重：根据梯度更新权重

4. ppt 上有一个显示的流程

5. 为什么越深的神经网络越好呢？

   - 因为可以做模块化的学习（比如第一层学习轮廓、第二层学习眼睛等）
   - 而且不要想着一次性规划所有的任务
   - 这样模块化的分类可以使用更少的数据进行训练

6. 深度学习 deep learning

   - 通过数据，学习高层次的抽象
   - 通过多层次的特征提取，学习数据的高层次表示

7. 深度学习的表征：

   - 传统的：通过人工和专家知识来设计特征
   - 深度学习：通过数据来学习特征（通过训练学出来的特征）
   - 同时，深度学习学到的是层次化的特征（把数据逐层地去抽象）
   - 深度学习把原有的机器学习的特征提取和分类的过程合并到了一起（用一个黑盒）
   - 因为数据变多了、所以深度学习的效果会更好

8. 缺点：原有的 Feedforward DNN 非常难以训练

9. 下面开始讲 Language model

10. 首先：让计算机理解单词

    - 一开始，让单词变成编码（字典）
    - 但是这个符号并不能代表计算机能够理解单词
    - 因为这个编码没有意义
    - 因此有了 One-Hot Encoding：就是把单词变成一个向量（只有一个 1，其他都是 0）
    - 但是单词太多了、这个向量太稀疏了、因此引入了 Word Embedding
    - 因此这个 embedding 是基于：Two words are similar if they have similar word contexts
